{"cells":[{"cell_type":"code","source":["lesson_name = \"jobs_demo\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75817ab6-23dc-4881-9017-72e1b02a710a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class JobConfig():\n    def __init__(self, job_name, notebook):\n        self.job_name = job_name\n        self.notebook = notebook\n    \n    def __repr__(self):\n        content =  f\"Name:      {self.job_name}\"\n        content += f\"Notebooks: {self.notebook}\"\n        return content\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2e57b609-25da-446d-82f6-7b03875ae954","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef get_job_config(self):\n    \n    job_name = f\"{DA.unique_name}: Example Job\"\n    \n    parts = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None).split(\"/\")[:-1]\n    notebook = \"/\".join(parts) + \"/DE 5.1.2 - Reset\"\n\n    return JobConfig(job_name, notebook)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9f557c53-be66-4760-8481-c455f8b8830b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef update_cluster_params(self, params: dict, task_indexes: list):\n\n    if not self.is_smoke_test():\n        return params\n    \n    for task_index in task_indexes:\n        # Need to modify the parameters to run run as a smoke-test.\n        task = params.get(\"tasks\")[task_index]\n        del task[\"existing_cluster_id\"]\n\n        cluster_params =         {\n            \"num_workers\": \"0\",\n            \"spark_version\": self.client.clusters().get_current_spark_version(),\n            \"spark_conf\": {\n              \"spark.master\": \"local[*]\"\n            },\n        }\n\n        instance_pool_id = self.client.clusters().get_current_instance_pool_id()\n        if instance_pool_id is not None: cluster_params[\"instance_pool_id\"] = self.client.clusters().get_current_instance_pool_id()\n        else:                            cluster_params[\"node_type_id\"] = self.client.clusters().get_current_node_type_id()\n\n        task[\"new_cluster\"] = cluster_params\n        \n    return params\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bb41343a-b231-48ed-85a7-6c2c8b35bce5","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef print_job_config_v1(self):\n    \"Provided by DBAcademy, this function renders the configuration of the job as HTML\"\n    job_config = self.get_job_config()\n    \n    displayHTML(f\"\"\"<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Job Name:</td>\n        <td><input type=\"text\" value=\"{job_config.job_name}\" style=\"width:100%\"></td></tr>\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Reset Notebook Path:</td>\n        <td><input type=\"text\" value=\"{job_config.notebook}\" style=\"width:100%\"></td></tr>\n\n    </table>\"\"\")    \n    "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b00ce6fb-5547-4e9a-99c5-798012301b23","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef create_job_v1(self):\n    \"Provided by DBAcademy, this function creates the prescribed job\"\n    import re\n    \n    job_config = self.get_job_config()\n\n    self.client.jobs.delete_by_name(job_config.job_name, success_only=False)\n    cluster_id = dbgems.get_tags().get(\"clusterId\")\n\n    build_name = re.sub(r\"[^a-zA-Z\\d]\", \"-\", self.course_config.course_name)\n    while \"--\" in build_name: build_name = build_name.replace(\"--\", \"-\")\n    \n    params = {\n        \"name\": job_config.job_name,\n        \"tags\": {\n            \"dbacademy.course\": build_name,\n            \"dbacademy.source\": build_name\n        },\n        \"email_notifications\": {},\n        \"timeout_seconds\": 7200,\n        \"max_concurrent_runs\": 1,\n        \"format\": \"MULTI_TASK\",\n        \"tasks\": [\n            {\n                \"task_key\": \"Reset\",\n                \"libraries\": [],\n                \"notebook_task\": {\n                    \"notebook_path\": job_config.notebook,\n                    \"base_parameters\": []\n                },\n                \"existing_cluster_id\": cluster_id\n            },\n        ],\n    }\n    params = self.update_cluster_params(params, [0])\n    \n    create_response = self.client.jobs().create(params)\n    job_id = create_response.get(\"job_id\")\n    \n    print(f\"Created job #{job_id}\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9ac7e4db-94a2-47dc-a193-3c5c4ee777ca","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef validate_job_v1_config(self):\n    \"Provided by DBAcademy, this function validates the configuration of the job\"\n    import json\n    \n    job_config = self.get_job_config()\n\n    job = self.client.jobs.get_by_name(job_config.job_name)\n    assert job is not None, f\"The job named \\\"{job_config.job_name}\\\" doesn't exist. Double check the spelling.\"\n\n    # print(json.dumps(job, indent=4))\n    \n    settings = job.get(\"settings\")\n    \n    if settings.get(\"format\") == \"SINGLE_TASK\":\n        notebook_path = settings.get(\"notebook_task\", {}).get(\"notebook_path\")\n        actual_cluster_id = settings.get(\"existing_cluster_id\", None)\n        #task_key = settings.get(\"task_key\", None)\n    else:\n        tasks = settings.get(\"tasks\", [])\n        assert len(tasks) == 1, f\"Expected one task, found {len(tasks)}.\"\n\n        notebook_path = tasks[0].get(\"notebook_task\", {}).get(\"notebook_path\")\n        actual_cluster_id = tasks[0].get(\"existing_cluster_id\", None)\n        \n        task_key = tasks[0].get(\"task_key\", None)\n        assert task_key == \"Rest\", f\"Expected the first task to have the name \\\"Reset\\\", found \\\"{task_key}\\\"\"\n        \n        \n    assert notebook_path == job_config.notebook, f\"Invalid Notebook Path. Found \\\"{notebook_path}\\\", expected \\\"{job_config.reset_notebook}\\\" \"\n    \n    if not self.is_smoke_test():\n        # Don't check the actual_cluster_id when running as a smoke test\n        \n        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n\n        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n        if expected_cluster_id != actual_cluster_id:\n            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n        \n    print(\"All tests passed!\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"445f434b-52d5-428d-b90c-dce3e821d5b1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef create_job_v2(self):\n    \"Provided by DBAcademy, this function creates the prescribed job\"\n    import re\n    \n    job_config = self.get_job_config()\n    pipeline_config = self.get_pipeline_config()\n\n    self.client.jobs.delete_by_name(job_config.job_name, success_only=False)\n    cluster_id = dbgems.get_tags().get(\"clusterId\")\n    \n    pipeline = self.client.pipelines().get_by_name(pipeline_config.pipeline_name)\n    pipeline_id = pipeline.get(\"pipeline_id\")\n    \n    build_name = re.sub(r\"[^a-zA-Z\\d]\", \"-\", self.course_config.course_name)\n    while \"--\" in build_name: build_name = build_name.replace(\"--\", \"-\")\n\n    params = {\n        \"name\": job_config.job_name,\n        \"tags\": {\n            \"dbacademy.course\": build_name,\n            \"dbacademy.source\": build_name\n        },\n        \"email_notifications\": {},\n        \"timeout_seconds\": 7200,\n        \"max_concurrent_runs\": 1,\n        \"format\": \"MULTI_TASK\",\n        \"tasks\": [\n            {\n                \"task_key\": \"Reset\",\n                \"libraries\": [],\n                \"notebook_task\": {\n                    \"notebook_path\": job_config.notebook,\n                    \"base_parameters\": []\n                },\n                \"existing_cluster_id\": cluster_id\n            },\n            {\n                \"task_key\": \"DLT\",\n                \"depends_on\": [ { \"task_key\": \"Reset\" } ],\n                \"pipeline_task\": {\n                    \"pipeline_id\": pipeline_id\n                },\n            },\n        ],\n    }\n    params = self.update_cluster_params(params, [0])\n    \n    create_response = self.client.jobs().create(params)\n    job_id = create_response.get(\"job_id\")\n    \n    print(f\"Created job #{job_id}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7ffe80ef-e079-45bf-8462-54bd42113439","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef validate_job_v2_config(self):\n    \"Provided by DBAcademy, this function validates the configuration of the job\"\n    import json\n    \n    pipeline_config = self.get_pipeline_config()\n    job_config = self.get_job_config()\n\n    job = self.client.jobs.get_by_name(job_config.job_name)\n    assert job is not None, f\"The job named \\\"{job_config.job_name}\\\" doesn't exist. Double check the spelling.\"\n    \n    settings = job.get(\"settings\")\n    assert settings.get(\"format\") == \"MULTI_TASK\", f\"Expected two tasks, found 1.\"\n\n    tasks = settings.get(\"tasks\", [])\n    assert len(tasks) == 2, f\"Expected two tasks, found {len(tasks)}.\"\n    \n    \n    # Reset Task\n    task_name = tasks[0].get(\"task_key\", None)\n    assert task_name == \"Reset\", f\"Expected the first task to have the name \\\"Reset\\\", found \\\"{task_name}\\\"\"\n    \n    notebook_path = tasks[0].get(\"notebook_task\", {}).get(\"notebook_path\")\n    assert notebook_path == job_config.notebook, f\"Invalid Notebook Path for the first task. Found \\\"{notebook_path}\\\", expected \\\"{job_config.notebook}\\\" \"\n\n    if not self.is_smoke_test():\n        # Don't check the actual_cluster_id when running as a smoke test\n        \n        actual_cluster_id = tasks[0].get(\"existing_cluster_id\", None)\n        assert actual_cluster_id is not None, f\"The first task is not configured to use the current All-Purpose cluster\"\n\n        expected_cluster_id = dbgems.get_tags().get(\"clusterId\")\n        if expected_cluster_id != actual_cluster_id:\n            actual_cluster = self.client.clusters.get(actual_cluster_id).get(\"cluster_name\")\n            expected_cluster = self.client.clusters.get(expected_cluster_id).get(\"cluster_name\")\n            assert actual_cluster_id == expected_cluster_id, f\"The first task is not configured to use the current All-Purpose cluster, expected \\\"{expected_cluster}\\\", found \\\"{actual_cluster}\\\"\"\n\n    \n    \n    # Reset DLT\n    task_name = tasks[1].get(\"task_key\", None)\n    assert task_name == \"DLT\", f\"Expected the second task to have the name \\\"DLT\\\", found \\\"{task_name}\\\"\"\n\n    actual_pipeline_id = tasks[1].get(\"pipeline_task\", {}).get(\"pipeline_id\", None)\n    assert actual_pipeline_id is not None, f\"The second task is not configured to use a Delta Live Tables pipeline\"\n    \n    expected_pipeline = self.client.pipelines().get_by_name(pipeline_config.pipeline_name)\n    actual_pipeline = self.client.pipelines().get_by_id(actual_pipeline_id)\n    actual_name = actual_pipeline.get(\"spec\").get(\"name\", \"Oops\")\n    assert actual_pipeline_id == expected_pipeline.get(\"pipeline_id\"), f\"The second task is not configured to use the correct pipeline, expected \\\"{pipeline_name}\\\", found \\\"{actual_name}\\\"\"\n    \n    depends_on = tasks[1].get(\"depends_on\", [])\n    assert len(depends_on) > 0, f\"The \\\"DLT\\\" task does not depend on the \\\"Reset\\\" task\"\n    assert len(depends_on) == 1, f\"The \\\"DLT\\\" task depends on more than just the \\\"Reset\\\" task\"\n    depends_task_key = depends_on[0].get(\"task_key\")\n    assert depends_task_key == \"Reset\", f\"The \\\"DLT\\\" task doesn't depend on the \\\"Reset\\\" task, found {depends_task_key}\"\n    \n    print(\"All tests passed!\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9253647-19d6-477d-9109-7c7e13c82c1a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef start_job(self):\n    job_config = self.get_job_config()\n    job_id = self.client.jobs.get_by_name(job_config.job_name).get(\"job_id\")\n    run_id = self.client.jobs.run_now(job_id).get(\"run_id\")\n    print(f\"Started job #{job_id}, run #{run_id}\")\n\n    self.client.runs.wait_for(run_id)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36a79b7b-8b49-4464-b184-14339cf18b48","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# The DataFactory is just a pattern to demonstrate a fake stream is more of a function\n# streaming workloads than it is of a pipeline - this pipeline happens to stream data.\nclass DataFactory:\n    def __init__(self):\n        \n        # Bind the stream-source to DA because we will use it again later.\n        DA.paths.stream_source = f\"{DA.paths.working_dir}/stream-source\"\n        \n        self.source_dir = f\"{DA.paths.datasets}/retail-pipeline\"\n        self.target_dir = DA.paths.stream_source\n        \n        # All three datasets *should* have the same count, but just in case,\n        # We are going to take the smaller count of the three datasets\n        orders_count = len(dbutils.fs.ls(f\"{self.source_dir}/orders/stream_json\"))\n        status_count = len(dbutils.fs.ls(f\"{self.source_dir}/status/stream_json\"))\n        customer_count = len(dbutils.fs.ls(f\"{self.source_dir}/customers/stream_json\"))\n        self.max_batch = min(min(orders_count, status_count), customer_count)\n        \n        self.current_batch = 0\n        \n    def load(self, continuous=False, delay_seconds=5):\n        import time\n        self.start = int(time.time())\n        \n        if self.current_batch >= self.max_batch:\n            print(\"Data source exhausted\\n\")\n            return False\n        elif continuous:\n            while self.load():\n                time.sleep(delay_seconds)\n            return False\n        else:\n            print(f\"Loading batch {self.current_batch+1} of {self.max_batch}\", end=\"...\")\n            self.copy_file(\"customers\")\n            self.copy_file(\"orders\")\n            self.copy_file(\"status\")\n            self.current_batch += 1\n            print(f\"{int(time.time())-self.start} seconds\")\n            return True\n            \n    def copy_file(self, dataset_name):\n        source_file = f\"{self.source_dir}/{dataset_name}/stream_json/{self.current_batch:02}.json/\"\n        target_file = f\"{self.target_dir}/{dataset_name}/{self.current_batch:02}.json\"\n        dbutils.fs.cp(source_file, target_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50d7b4f1-1b5d-4ef2-a126-6252a8ef832d","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["class PipelineConfig():\n    def __init__(self, pipeline_name, notebook):\n        self.pipeline_name = pipeline_name  # The name of the pipeline\n        self.notebook = notebook            # This list of notebooks for this pipeline\n    \n    def __repr__(self):\n        content =  f\"Name:     {self.pipeline_name}\\n\"\n        content += f\"Notebook: {self.notebook}\"\n        return content\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5b84ee85-fdff-46a2-be20-ff6374357f88","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef get_pipeline_config(self):\n    path = dbutils.entry_point.getDbutils().notebook().getContext().notebookPath().getOrElse(None)\n    notebook = \"/\".join(path.split(\"/\")[:-1]) + \"/DE 5.1.3 - DLT Job\"\n    \n    pipeline_name = f\"{DA.unique_name}: Pipeline Demo w/Job\"\n    \n    return PipelineConfig(pipeline_name, notebook)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ce80785c-e396-4e47-8a57-63034c156bf8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["@DBAcademyHelper.monkey_patch\ndef create_pipeline(self):\n    \"\"\"\n    Creates the prescribed pipeline.\n    \"\"\"\n    \n    from dbacademy.dbrest import DBAcademyRestClient\n    client = DBAcademyRestClient()\n\n    config = self.get_pipeline_config()\n    print(f\"Creating the pipeline \\\"{config.pipeline_name}\\\"\")\n\n    # Delete the existing pipeline if it exists\n    client.pipelines().delete_by_name(config.pipeline_name)\n\n    \n    # Create the new pipeline\n    pipeline = client.pipelines().create(\n        name = config.pipeline_name, \n        development=True,\n        storage = self.paths.storage_location, \n        target = self.schema_name, \n        notebooks = [config.notebook], \n        configuration={\n            \"source\": DA.paths.datasets,\n        })\n    \n    self.pipeline_id = pipeline.get(\"pipeline_id\")\n\n    policy = self.client.cluster_policies.get_by_name(\"DBAcademy DLT-Only Policy\")\n    if policy is not None:\n        self.client.pipelines.create_or_update(name = config.pipeline_name,\n                                               storage = self.paths.storage_location,\n                                               target = self.schema_name,\n                                               notebooks = [config.notebook],\n                                               configuration = {\n                                                    \"source\": DA.paths.datasets,\n                                                    \"spark.master\": \"local[*]\",\n                                                },\n                                                clusters=[{ \n                                                    \"num_workers\": 0,\n                                                    \"label\": \"default\", \n                                                    \"policy_id\": policy.get(\"policy_id\")\n                                                }])\n    \n    displayHTML(f\"\"\"<table style=\"width:100%\">\n    <tr>\n        <td style=\"white-space:nowrap; width:1em\">Pipeline Name:</td>\n        <td><input type=\"text\" value=\"{config.pipeline_name}\" style=\"width:100%\"></td></tr>\n\n    </table>\"\"\")    \n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"face02f4-8f9f-4dad-a6a4-d50a6e65bcbb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Classroom-Setup-05.1-Common","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4060780405477270}},"nbformat":4,"nbformat_minor":0}
